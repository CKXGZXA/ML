{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四题：神经网络：三层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现内容：\n",
    "1. 实现一个三层感知机\n",
    "2. 对手写数字数据集进行分类\n",
    "3. 绘制损失值变化曲线\n",
    "4. 完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这道题中，我们要实现一个三层感知机\n",
    "\n",
    "<img src=\"https://davidham3.github.io/blog/2018/09/11/logistic-regression/Fig2.png\" ,width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "\n",
    "我们实现一个最简单的三层感知机，一个输入层，一个隐藏层，一个输出层，隐藏层单元个数为$h$个，输出层有$K$个单元。\n",
    "\n",
    "1. 我们将第一层的输入，定义为$X \\in \\mathbb{R}^{n \\times m}$，n个样本，m个特征。  \n",
    "2. 输入层到隐藏层之间的权重(weight)与偏置(bias)，分别为$W_1 \\in \\mathbb{R}^{m \\times h}$，$b_1 \\in \\mathbb{R}^{1 \\times h}$。  \n",
    "3. 隐藏层到输出层的权重和偏置分为别$W_2 \\in \\mathbb{R}^{h \\times K}$，$b_2 \\in \\mathbb{R}^{1 \\times K}$。\n",
    "\n",
    "隐藏层的激活函数选用ReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\max (0, x)\n",
    "$$\n",
    "\n",
    "我们用$H_1$表示第一个隐藏层的输出值，$O$表示输出层的输出值，这样，前向传播即可定义为\n",
    "\n",
    "$$\n",
    "Z = XW_1 + b_1\\\\\n",
    "H_1 = \\mathrm{ReLU}(Z)\\\\\n",
    "O = H_1 W_2 + b_2\n",
    "$$\n",
    "\n",
    "其中，$H_1 \\in \\mathbb{R}^{n \\times h}$，$O \\in \\mathbb{R}^{n \\times K}$。\n",
    "\n",
    "**注意：这里我们其实是做了广播，将$b_1$复制了$n-1$份后拼接成了维数为$n \\times h$的矩阵，同理，$b_2$也做了广播，拼成了$n \\times K$的矩阵。**\n",
    "\n",
    "最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y_i} & = \\mathrm{softmax}(O_i)\\\\\n",
    "& = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\hat{y_i}$表示第$i$类的概率值，也就是输出层第$i$个神经元经$\\mathrm{softmax}$激活后的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "损失函数使用交叉熵损失函数：\n",
    "$$\\mathrm{cross\\_entropy}(y, \\hat{y}) = -\\sum^{K}_{k=1}y_k \\log{(\\hat{y_k})}$$\n",
    "\n",
    "这样，$n$个样本的平均损失为：\n",
    "$$\n",
    "\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\n",
    "$$\n",
    "\n",
    "**注意，这里我们的提到的$\\log$均为$\\ln$，在numpy中为**`np.log`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "\n",
    "我们使用梯度下降训练模型，求解方式就是求出损失函数对参数的偏导数，即参数的梯度，然后将参数减去梯度乘以学习率，进行参数的更新。\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "其中，$\\alpha$是学习率。\n",
    "\n",
    "在这道题中，交叉熵损失函数的求导比较麻烦，我们先求神经网络的输出层的偏导数，写成链式法则的形式：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}\n",
    "$$\n",
    "\n",
    "首先求解第一项：\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}\n",
    "$$\n",
    "\n",
    "然后求解第二项，因为$\\hat{y_k}$的分母是$\\sum_k \\exp{(O_k)}$，里面包含$O_i$，所以每一个$\\hat{y_k}$的分母都包含$O_i$，这就要求反向传播的时候需要考虑这$K$项，将这$K$项的偏导数加在一起。\n",
    "\n",
    "这$K$项分别为：$\\frac{\\exp{(O_1)}}{\\sum_k \\exp{(O_k)}}$，$\\frac{\\exp{(O_2)}}{\\sum_k \\exp{(O_k)}}$，...，$\\frac{\\exp{(O_i)}}{\\sum_k \\exp{(O_k)}}$，...，$\\frac{\\exp{(O_k)}}{\\sum_k \\exp{(O_k)}}$。\n",
    "\n",
    "显然，这里只有分子带有$O_i$的这项与其他的项不同，因为分子和分母同时包含了$O_i$，而其他的项只有分母包含了$O_i$。\n",
    "\n",
    "这就需要在求解$\\frac{\\partial \\hat{y}}{\\partial O_i}$的时候分两种情况讨论\n",
    "1. 分子带$O_i$\n",
    "2. 分子不带$O_i$\n",
    "\n",
    "第一种情况，当分子含有$O_i$时：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y_i}}{\\partial O_i} & = \\frac{\\partial \\hat{y_i}}{\\partial O_i}\\\\\n",
    "& = \\frac{\\exp{(O_i)} (\\sum^{K}_{k=1} \\exp{(O_k)}) - (\\exp{(O_i)})^2 }{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\\n",
    "& = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}} \\frac{\\sum^{K}_{k=1} \\exp{(O_k)} - \\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\\\\\n",
    "& = \\hat{y_i} ( 1 - \\hat{y_i} )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "第二种情况，当分子不含$O_i$时，我们用$j$表示当前项的下标：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y_j}}{\\partial O_i} & = \\frac{- \\exp{(O_j)} \\exp{(O_i)}}{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\\n",
    "& = - \\hat{y_j} \\hat{y_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这样，$\\mathrm{loss}$对$O_i$的偏导数即为：\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\\n",
    "& = (- \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}) \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\\n",
    "& = - \\frac{1}{n} \\sum_n (y_i \\frac{1}{\\hat{y_i}} \\hat{y_i} ( 1 - \\hat{y_i} ) + \\sum^K_{k \\not= i} y_k \\frac{1}{\\hat{y_k}}( - \\hat{y_k} \\hat{y_i}))\\\\\n",
    "& = - \\frac{1}{n} \\sum_n ( y_i - y_i \\hat{y_i} - \\sum^K_{k \\not= i} y_k \\hat{y_i})\\\\\n",
    "& = - \\frac{1}{n} \\sum_n ( y_i  - \\hat{y_i} \\sum^K_{k = 1} y_k )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于我们处理的多类分类任务，一个样本只对应一个标记，所以$\\sum^K_{k = 1} y_k = 1$，上式在这种问题中，即可化简为：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} &= - \\frac{1}{n} \\sum_n ( y_i  - \\hat{y_i})\\\\\n",
    "& = \\frac{1}{n} \\sum_n (\\hat{y_i} -  y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "将其写成矩阵表达式：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O} &= \\frac{1}{n} (\\hat{y} - y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "也就是说，我们的损失函数对输出层的$K$个神经单元的偏导数为$\\mathrm{softmax}$激活值减去真值。\n",
    "\n",
    "接下来我们需要求损失函数对参数$W_2$和$b_2$的偏导数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial W_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial b_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\frac{\\partial loss}{\\partial W_2} \\in \\mathbb{R}^{h \\times K}$，$\\frac{\\partial loss}{\\partial b_2} \\in \\mathbb{R}^{1 \\times K}$。  \n",
    "**注意，由于$b_2$是被广播成$n \\times K$的矩阵，因此实际上$b_2$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。**\n",
    "\n",
    "同理，我们可以求得$\\mathrm{loss}$对$W_1$和$b_1$的偏导数：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial W_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\\n",
    "& = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于我们使用的是$\\mathrm{ReLU}$激活函数，它的偏导数为：\n",
    "\n",
    "$$\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } x < 0\\\\\n",
    "1 & \\text{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "所以上式为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial {W_1}_{ij}} =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } {Z}_{ij} < 0\\\\\n",
    "    \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} & \\text{if } {Z}_{ij} \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中，${W_1}_{ij}$表示矩阵$W_1$第$i$行第$j$列的值，${Z}_{ij}$表示矩阵$Z$第$i$行第$j$列的值。  \n",
    "同理：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial b_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\\n",
    "& = \\begin{cases}\n",
    "0 &\\text{if } {Z}_{ij} < 0\\\\\n",
    "\\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &\\text{if } {Z}_{ij} \\geq 0\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\frac{\\partial loss}{\\partial W_1} \\in \\mathbb{R}^{m \\times h}$，$\\frac{\\partial loss}{\\partial b_1} \\in \\mathbb{R}^{1 \\times h}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数更新\n",
    "\n",
    "求得损失函数对四个参数的偏导数后，我们就可以使用梯度下降进行参数更新：\n",
    "$$\n",
    "W_2 := W_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_2}\\\\\n",
    "b_2 := b_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_2}\\\\\n",
    "W_1 := W_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_1}\\\\\n",
    "b_1 := b_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_1}\\\\\n",
    "$$\n",
    "其中，$\\alpha$是学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上内容，就是一个三层感知机的前向传播与反向传播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用第一题的手写数字数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40%做测试集，60%做训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(load_digits()['data'], load_digits()['target'], test_size = 0.4, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1078, 64), (1078,), (719, 64), (719,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用和第一题一样的标准化处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "trainX = s.fit_transform(trainX)\n",
    "testX = s.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来还要处理输出。  \n",
    "我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。  \n",
    "我们当前的trainY和testY，每个样本都是一个类标，我们需要将其变成one_hot编码，也就是，假设当前样本的类别是3，我们需要把它变成一个长度为10的向量，其中第4个元素为1，其他元素都为0。得到的矩阵分别记为trainY_mat和testY_mat。  \n",
    "这样，模型训练完成后，会针对每个样本输出十个数，分别代表这个样本属于$0,1,...,9$的概率，那我们只要取最大的那个数的下标，就知道模型认为这个样本是哪类了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainY_mat = np.zeros((len(trainY), 10))\n",
    "trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n",
    "\n",
    "testY_mat = np.zeros((len(testY), 10))\n",
    "testY_mat[np.arange(0, len(testY), 1), testY] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1078, 10), (719, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_mat.shape, testY_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这题和上一题的区别是，我们把参数用dict存起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize(h, K):\n",
    "    '''\n",
    "    参数初始化\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    h: int: 隐藏层单元个数\n",
    "    \n",
    "    K: int: 输出层单元个数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    parameters: dict，参数，键是\"W1\", \"b1\", \"W2\", \"b2\"\n",
    "    \n",
    "    '''\n",
    "    np.random.seed(32)\n",
    "    W_1 = np.random.normal(size = (trainX.shape[1], h)) * 0.01\n",
    "    b_1 = np.zeros((1, h))\n",
    "    \n",
    "    np.random.seed(32)\n",
    "    W_2 = np.random.normal(size = (h, K)) * 0.01\n",
    "    b_2 = np.zeros((1, K))\n",
    "    \n",
    "    parameters = {'W1': W_1, 'b1': b_1, 'W2': W_2, 'b2': b_2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50)\n",
      "(1, 50)\n",
      "(50, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].shape) # (64, 50)\n",
    "print(parameterst['b1'].shape) # (1, 50)\n",
    "print(parameterst['W2'].shape) # (50, 10)\n",
    "print(parameterst['b2'].shape) # (1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成Z的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_combination(X, W, b):\n",
    "    '''\n",
    "    计算Z，Z = XW + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    W: np.ndarray, shape = (m, h)，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, h)，偏置\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Z: np.ndarray, shape = (n, h)，线性组合后的值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Z = XW + b\n",
    "    # YOUR CODE HERE\n",
    "    Z = X@W + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 50)\n",
      "-5.273044421225233e-19\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "print(Zt.shape) # (1078, 50)\n",
    "print(Zt.mean()) # -5.27304442123e-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rm ReLU$激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    '''\n",
    "    ReLU激活函数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    activations: np.ndarray, 激活后的矩阵\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    X[X < 0] = 0\n",
    "    activations = X\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030445670920609378\n",
      "(1078, 10)\n",
      "0.0006001926584638006\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "print(Ht.mean()) # 0.0304\n",
    "\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "print(Ot.shape) # (1078, 10)\n",
    "print(Ot.mean()) # 0.0006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rm softmax$激活  \n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(O_i) = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def my_softmax(O):\n",
    "    '''\n",
    "    softmax激活\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return np.exp(O) / np.sum(np.exp(O), axis = 1, keepdims = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]]\n",
      "[[nan nan nan]]\n",
      "[[nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CKXG\\AppData\\Local\\Temp/ipykernel_19128/2870592161.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(O) / np.sum(np.exp(O), axis = 1, keepdims = True)\n",
      "C:\\Users\\CKXG\\AppData\\Local\\Temp/ipykernel_19128/2870592161.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(O) / np.sum(np.exp(O), axis = 1, keepdims = True)\n"
     ]
    }
   ],
   "source": [
    "# 测试样例1\n",
    "print(my_softmax(np.array([[0.3, 0.3, 0.3]])))  # array([[ 0.33333333,  0.33333333,  0.33333333]])\n",
    "\n",
    "# 测试样例2\n",
    "test1 = np.array([[-1e32, -1e32, -1e32]])\n",
    "test2 = np.array([[1e32, 1e32, 1e32]])\n",
    "print(my_softmax(test1))\n",
    "print(my_softmax(test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，其实是有数值计算上的问题的，假设，我们最后的输出有三个数，每个数都特别小，理论上来说，通过$\\rm softmax$激活后，三个值都是$\\frac{1}{3}$。但实际上就不是这样了，实际上会导致分母为0，除法就不能做了。如果每个数都特别大，会导致做指数运算的时候上溢。\n",
    "\n",
    "我们需要用其他的方法来实现$\\rm softmax$。\n",
    "\n",
    "我们将传入$\\rm softmax$的向量，每个元素减去他们中的最大值，即\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(O_i) = \\mathrm{softmax}(O_i - \\mathrm{max(O)})\n",
    "$$\n",
    "\n",
    "这个式子是成立的，感兴趣的同学可以证明一下上面的式子。\n",
    "\n",
    "当我们做了这样的变换后，向量$O$中的最大值就变成了0，就不会上溢了，而分母中最少有一项为1，也不会出现下溢导致分母为0的问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def softmax(O):\n",
    "    '''\n",
    "    softmax激活函数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    O: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    activations: np.ndarray, 激活后的矩阵\n",
    "    \n",
    "    '''\n",
    "    # YOUR CODE HEER\n",
    "    O = O - np.max(O, axis=1, keepdims=True)\n",
    "    activations = my_softmax(O)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "0.0006001926584638006\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_pred = softmax(Ot)\n",
    "\n",
    "print(y_pred.shape)  # (1078, 10)\n",
    "print(Ot.mean())     # 0.000600192658464\n",
    "print(y_pred.mean()) # 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是实现损失函数，交叉熵损失函数：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里又会出一个问题，交叉熵损失函数中，我们需要对$\\rm softmax$的激活值取对数，也就是$\\log{\\hat{y}}$，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的$\\rm softmax$在有些时候确实会输出0，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([[1e32, 0, -1e32]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就使得在计算loss的时候会出现问题，解决这个问题的方法是$\\rm log \\ softmax$。所谓$\\rm log \\ softmax$，就是将交叉熵中的对数运算与$\\rm softmax$结合起来，避开为0的情况\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\log{\\frac{\\exp{(O_i)}}{\\sum_K \\exp{(O_k)}}} &= \\log{\\frac{\\exp{(O_i - \\mathrm{max}(O))}}{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}}\\\\\n",
    "&= O_i - \\mathrm{max}(O) - \\log{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这样我们再计算$\\rm loss$的时候就可以把输出层的输出直接放到$\\rm log \\ softmax$中计算，不用先激活，再取对数了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先编写`log_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    '''\n",
    "    log softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    log_activations: np.ndarray, 激活后取了对数的矩阵\n",
    "    \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    log_activations = x - np.max(x) - np.log( np.sum(np.exp(x - np.max(x)), axis = 1, keepdims = True) )\n",
    "    \n",
    "    return log_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "-2.3025914871652615\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "t = log_softmax(Ot)\n",
    "print(t.shape)  # (1078, 10)\n",
    "print(t.mean()) # -2.30259148717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后编写`cross_entropy_with_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_with_softmax(y_true, O):\n",
    "    '''\n",
    "    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss: float, 平均的交叉熵损失值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 平均交叉熵损失\n",
    "    # YOUR CODE HERE\n",
    "    loss = - 1/len(y_true) * np.sum(np.sum(y_true * log_softmax(O)))    # 这里是元素乘\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3026670795819744\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "losst = cross_entropy_with_softmax(trainY_mat, Ot)\n",
    "print(losst.mean()) # 2.30266707958"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正是因为$\\rm softmax$激活与交叉熵损失会有这样的问题，所以在很多深度学习框架中，交叉熵损失函数就直接带有了激活的功能，所以我们在实现前向传播计算的时候，就不要加$\\rm softmax$激活函数了。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def forward(X, parameters):\n",
    "    '''\n",
    "    前向传播，从输入一直到输出层softmax激活前的值\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n",
    "    \n",
    "    '''\n",
    "    # 输入层到隐藏层\n",
    "    # YOUR CODE HERE\n",
    "    Z = X @ parameters['W1'] + parameters['b1']\n",
    "    \n",
    "    # 隐藏层的激活\n",
    "    # YOUR CODE HERE\n",
    "    H = ReLU(Z)\n",
    "    \n",
    "    # 隐藏层到输出层\n",
    "    # YOUR CODE HERE\n",
    "    O = H @ parameters['W2'] + parameters['b2']\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006001926584638006\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Ot = forward(trainX, parameterst)\n",
    "print(Ot.mean()) # 0.000600192658464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y_true, y_pred, H, Z, X, parameters):\n",
    "    '''\n",
    "    计算梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n",
    "    \n",
    "    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n",
    "    \n",
    "    X: np.ndarray, shape = (n, m)，输入的原始数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    grads: dict, 梯度\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 计算W2的梯度\n",
    "    # YOUR CODE HERE\n",
    "    dW2 = (1/len(y_true)) * np.dot(np.transpose(H), (y_pred - y_true))\n",
    "    \n",
    "    # 计算b2的梯度\n",
    "    # YOUR CODE HERE\n",
    "    db2 = (1/len(y_true)) * np.sum(y_pred - y_true, axis=0)\n",
    "    \n",
    "    # 计算ReLU的梯度\n",
    "    relu_grad = Z.copy()\n",
    "    relu_grad[relu_grad >= 0] = 1\n",
    "    relu_grad[relu_grad < 0] = 0\n",
    "    \n",
    "    # 计算W1的梯度\n",
    "    # YOUR CODE HERE\n",
    "    dW1 = 1/len(y_true) * np.dot(np.transpose(X), np.dot((y_pred - y_true), np.transpose(parameters['W2'])) * relu_grad  )\n",
    "    # 计算b1的梯度\n",
    "    # YOUR CODE HERE\n",
    "    db1 = 1/len(y_true) * np.sum(np.dot((y_pred - y_true), np.transpose(parameters['W2'])) * relu_grad, axis=0)\n",
    "    \n",
    "    grads = {'dW2': dW2, 'db2': db2, 'dW1': dW1, 'db1': db1}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04291861176681737\n",
      "-5.059851518574189e-05\n",
      "6.071532165918825e-18\n",
      "-2.6020852139652106e-17\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_predt = softmax(Ot)\n",
    "\n",
    "gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)\n",
    "\n",
    "print(gradst['dW1'].sum()) # 0.0429186117668\n",
    "print(gradst['db1'].sum()) # -5.05985151857e-05\n",
    "print(gradst['dW2'].sum()) # -2.16840434497e-18\n",
    "print(gradst['db2'].sum()) # -1.34441069388e-17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降，参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    参数更新\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: dict，参数\n",
    "    \n",
    "    grads: dict, 梯度\n",
    "    \n",
    "    learning_rate: float, 学习率\n",
    "    \n",
    "    '''\n",
    "    parameters['W2'] -= learning_rate * grads['dW2']\n",
    "    parameters['b2'] -= learning_rate * grads['db2']\n",
    "    parameters['W1'] -= learning_rate * grads['dW1']\n",
    "    parameters['b1'] -= learning_rate * grads['db1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播，参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834954544808615\n",
      "0.0\n",
      "0.18887164310031426\n",
      "0.0\n",
      "\n",
      "0.57920359330418\n",
      "5.059851518574182e-06\n",
      "0.18887164310031426\n",
      "2.439454888092385e-18\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].sum())  # 0.583495454481\n",
    "print(parameterst['b1'].sum())  # 0.0\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 0.0\n",
    "print()\n",
    "\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_predt = softmax(Ot)\n",
    "\n",
    "gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)\n",
    "update(parameterst, gradst, 0.1)\n",
    "\n",
    "print(parameterst['W1'].sum())  # 0.579203593304\n",
    "print(parameterst['b1'].sum())  # 5.05985151857e-06\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 1.24683249836e-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def backward(y_true, y_pred, H, Z, X, parameters, learning_rate):\n",
    "    '''\n",
    "    计算梯度，参数更新\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n",
    "    \n",
    "    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n",
    "    \n",
    "    X: np.ndarray, shape = (n, m)，输入的原始数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    learning_rate: float, 学习率\n",
    "    \n",
    "    '''\n",
    "    # 计算梯度\n",
    "    # YOUR CODE HERE\n",
    "    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)\n",
    "    \n",
    "    # 更新参数\n",
    "    # YOUR CODE HERE\n",
    "    update(parameters, grads, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834954544808615\n",
      "0.0\n",
      "0.18887164310031426\n",
      "0.0\n",
      "\n",
      "0.57920359330418\n",
      "5.059851518574182e-06\n",
      "0.18887164310031426\n",
      "2.439454888092385e-18\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].sum())  # 0.583495454481\n",
    "print(parameterst['b1'].sum())  # 0.0\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 0.0\n",
    "print()\n",
    "\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_predt = softmax(Ot)\n",
    "\n",
    "backward(trainY_mat, y_predt, Ht, Zt, trainX, parameterst, 0.1)\n",
    "\n",
    "print(parameterst['W1'].sum())  # 0.579203593304\n",
    "print(parameterst['b1'].sum())  # 5.05985151857e-06\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 1.24683249836e-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(trainX, trainY, testX, testY, parameters, epochs, learning_rate = 0.01, verbose = False):\n",
    "    '''\n",
    "    训练\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainX: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    trainY: np.ndarray, shape = (n, K), 训练集标记\n",
    "    \n",
    "    testX: np.ndarray, shape = (n_test, m)，测试集\n",
    "    \n",
    "    testY: np.ndarray, shape = (n_test, K)，测试集的标记\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    epochs: int, 要迭代的轮数\n",
    "    \n",
    "    learning_rate: float, default 0.01，学习率\n",
    "    \n",
    "    verbose: boolean, default False，是否打印损失值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    # 存储损失值\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵\n",
    "        Z = linear_combination(trainX, parameters['W1'], parameters['b1'])\n",
    "        H = ReLU(Z)\n",
    "        train_O = linear_combination(H, parameters['W2'], parameters['b2'])\n",
    "        train_y_pred = softmax(train_O)\n",
    "        training_loss = cross_entropy_with_softmax(trainY, train_O)\n",
    "        \n",
    "        test_O = forward(testX, parameters)\n",
    "        testing_loss = cross_entropy_with_softmax(testY, test_O)\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('epoch %s, training loss:%s'%(i + 1, training_loss))\n",
    "            print('epoch %s, testing loss:%s'%(i + 1, testing_loss))\n",
    "            print()\n",
    "        \n",
    "        training_loss_list.append(training_loss)\n",
    "        testing_loss_list.append(testing_loss)\n",
    "        \n",
    "        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)\n",
    "    return training_loss_list, testing_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834954544808615\n",
      "0.0\n",
      "0.18887164310031426\n",
      "0.0\n",
      "\n",
      "0.57920359330418\n",
      "5.059851518574182e-06\n",
      "0.18887164310031426\n",
      "2.439454888092385e-18\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].sum())  # 0.583495454481\n",
    "print(parameterst['b1'].sum())  # 0.0\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 0.0\n",
    "print()\n",
    "\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, 1, 0.1, False)\n",
    "\n",
    "print(parameterst['W1'].sum())  # 0.579203593304\n",
    "print(parameterst['b1'].sum())  # 5.05985151857e-06\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 1.24683249836e-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 绘制模型损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss_curve(training_loss_list, testing_loss_list):\n",
    "    '''\n",
    "    绘制损失值变化曲线\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(training_loss_list, label = 'training loss')\n",
    "    plt.plot(testing_loss_list, label = 'testing loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练完后，我们的就可以进行预测了，需要注意的是，我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    '''\n",
    "    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prediction: np.ndarray, shape = (n, 1)，预测的标记\n",
    "    \n",
    "    '''\n",
    "    # 用forward函数得到softmax激活前的值\n",
    "    # YOUR CODE HERE\n",
    "    O = forward(X, parameters)\n",
    "    \n",
    "    # 计算softmax激活后的值\n",
    "    # YOUR CODE HERE\n",
    "    y_pred = softmax(O)\n",
    "    \n",
    "    # 取每行最大的元素对应的下标\n",
    "    # YOUR CODE HERE\n",
    "    prediction = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1599443671766342"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试样例\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "parameterst = initialize(50, 10)\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, 1, 0.1, False)\n",
    "\n",
    "predictiont = predict(testX, parameterst)\n",
    "accuracy_score(predictiont, testY)  # 0.15994436717663421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 训练一个三层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏层单元数设置为50，输出层单元数为10，我们设置学习率为0.03，迭代轮数为1000轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 2.655444383621216 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "h = 50\n",
    "K = 10\n",
    "parameters = initialize(h, K)\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, 1000, 0.03, False)\n",
    "\n",
    "end_time = time()\n",
    "print('training time: %s s'%(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算测试集精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9499304589707928"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(testX, parameters)\n",
    "accuracy_score(prediction, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFzCAYAAAB2A95GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABIAklEQVR4nO3dd3hW9f3/8efnvrMH2RCSQMJeYYct4hZEwYkLB247tK21orXa8W1rf22ttW5cVXGP2haqOBEHeyMbkhAgkL13Pr8/zs3ekORkvB7XdV/nPuc+9+F9c183vvycz3kfY61FRERERJqWx+0CRERERNoihTARERERFyiEiYiIiLhAIUxERETEBQphIiIiIi5QCBMRERFxgZ/bBZyo2NhYm5KS4nYZIiIiIse0ZMmSXGtt3OFea3EhLCUlhcWLF7tdhoiIiMgxGWMyjvSaTkeKiIiIuEAhTERERMQFCmEiIiIiLmhxc8JERETkUDU1NWRlZVFZWel2KW1SUFAQSUlJ+Pv7H/d7FMJERERagaysLMLDw0lJScEY43Y5bYq1lry8PLKysujSpctxv0+nI0VERFqByspKYmJiFMBcYIwhJibmhEchFcJERERaCQUw95zM371CmIiIiJyywsJCnnrqqZN67wUXXEBhYeFR93nooYf49NNPT+r4B0tJSSE3N7dBjnUqFMJERETklB0thNXW1h71vbNnzyYyMvKo+/z2t7/lnHPOOdnymiWFMBERETll06dPZ/PmzQwaNIh7772XL7/8krFjxzJp0iT69u0LwMUXX8zQoUPp168fzz333N737hmZSk9Pp0+fPtx6663069eP8847j4qKCgBuvPFG3n333b37P/zwwwwZMoT+/fuzbt06AHJycjj33HPp168ft9xyC8nJyccc8Xr00UdJTU0lNTWVxx57DICysjImTpzIwIEDSU1N5a233tr7Gfv27cuAAQP4+c9/fsp/Z7o6UkREpJX5zX/W8P2O4gY9Zt+Edjx8Ub8jvv7II4+wevVqli9fDsCXX37J0qVLWb169d4rBl988UWio6OpqKhg2LBhXHbZZcTExBxwnI0bN/LGG28wY8YMpkyZwnvvvcfUqVMP+fNiY2NZunQpTz31FH/5y194/vnn+c1vfsNZZ53F/fffz0cffcQLL7xw1M+0ZMkSXnrpJRYsWIC1lhEjRjBu3Di2bNlCQkICs2bNAqCoqIi8vDw++OAD1q1bhzHmmKdPj4dGwg5SWlLI8i/eZe3iz9m2YTmFu7Ooq1bPFRERkRM1fPjwA1o2PP744wwcOJCRI0eybds2Nm7ceMh7unTpwqBBgwAYOnQo6enphz32pZdeesg+X3/9NVdddRUA48ePJyoq6qj1ff3111xyySWEhoYSFhbGpZdeyrx58+jfvz+ffPIJ9913H/PmzSMiIoKIiAiCgoK4+eabef/99wkJCTnBv41DaSTsINlb1jBo7s2HbK/EnzITRrknjCpvGDX+4dQGtMMGRmCCI/AER+IfGkVgWBRhkbG0i03Av108hMaCx+vCJxERkbbqaCNWTSk0NHTv8y+//JJPP/2U7777jpCQEM4444zDtnQIDAzc+9zr9e49HXmk/bxe7zHnnJ2onj17snTpUmbPns2DDz7I2WefzUMPPcTChQv57LPPePfdd3niiSf4/PPPT+nPUQg7SEK3VDZMfI+K0nyqS/OpLSukrrwIKgsxVcX4VRcTUFtCYEU+4aXbCKOMdpQRYOoOe7w6PJR62lEeEEN1UCw2JA6/qCRC47sREd8NT1QyRCSBf1ATf1IREZGGEx4eTklJyRFfLyoqIioqipCQENatW8f8+fMbvIYxY8bw9ttvc9999zFnzhwKCgqOuv/YsWO58cYbmT59OtZaPvjgA1599VV27NhBdHQ0U6dOJTIykueff57S0lLKy8u54IILGDNmDF27dj3lehXCDhISFkHPYcd/9UVNXT1F5dUUFZdQWpRLWVE+ZYU5VBZmU1+SjSnLwa8il9CqXNqV5xFXsJkO2wvwX3NgaCvxj6UivDPeDv1o17k//vF9oX0fZyRNRESkmYuJiWHMmDGkpqYyYcIEJk6ceMDr48eP55lnnqFPnz706tWLkSNHNngNDz/8MFdffTWvvvoqo0aNIj4+nvDw8CPuP2TIEG688UaGDx8OwC233MLgwYP5+OOPuffee/F4PPj7+/P0009TUlLC5MmTqaysxFrLo48+esr1GmvtKR+kKaWlpdnFixe7XcZJqa6tZ1dxJdtyS9i9I4OSXZupzUvHW7yN0PLtJLODniaLdqZ873vKA+KoSRhKeLdReDoNg4RBEBB65D9ERETapLVr19KnTx+3y3BVVVUVXq8XPz8/vvvuO+688869Fwo0hcN9B8aYJdbatMPtr5GwJhTg56FTdAidokOgZwdg+N7X6ustGfnlfL2jiG0ZmynLWo03dx2dKjYxZMsyItI/cvYzXqo6DCGo97mY7uc4oUxzzkRERMjMzGTKlCnU19cTEBDAjBkz3C7pqBTCmgmPx9AlNpQusaEwIAEYC8D2wgoWp+fz+qbNlG9eSMeSFZy2YzX9s/+I+fIP1AZG4u19Aab/ZdBlHHiP/+7tIiIirUmPHj1YtmyZ22UcN4WwZi4xMpjEQYlMHpQInE5WQTlfrM/hxTUb8MuYy+jyZZy/4l+ErXiduqBovKmXQNo0iO/vdukiIiJyFAphLUxSVAjXjUzmupHJlFWdySff7+LupVvxbvmMiXXzmbDkVQIWv4BNGo4Zfiv0u0SjYyIiIs2QQlgLFhrox8WDE7l4cCK7S9J4Z3EWE79dw9jyT7hx+2d0fv9W7Of/hzntpzDoGvALPPZBRUREpEmoY34r0T48iB+e2Z3Z0y9i0JQHuD3iWW6q/jnrigPgvz/BPj4YVrwJ9fVulyoiIiIohLU6/l4PkwYmMOuusVx+9S3cFfoXplbfz+aKEPjgdnjhXNi2yO0yRUSklSksLOSpp5466fc/9thjlJfva9F0wQUXNMj9GdPT00lNTT3l4zQGhbBWyuMxXNC/Ix/9dBwTL76GK2r/j3tr76Bkdzq8cA7M+jlUlbpdpoiItBINHcJmz55NZGRkA1TWfCmEtXJej+Hq4Z35/Odn4TfkWkaU/D/e878Iu+h5eHoUpH/tdokiItIKTJ8+nc2bNzNo0CDuvfdeAP785z8zbNgwBgwYwMMPPwxAWVkZEydOZODAgaSmpvLWW2/x+OOPs2PHDs4880zOPPNMAFJSUsjNzSU9PZ0+ffpw66230q9fP84777y995NctGgRAwYM2PtnHmvEq7KykmnTptG/f38GDx7MF198AcCaNWsYPnw4gwYNYsCAAWzcuPGwdTY0TcxvI6JCA/jjpQO4aEAC97wTyds1Q3i26kUi/nkR5owHYOw94FEmFxFpFf43HbJXNewx4/vDhEeO+PIjjzzC6tWr93aonzNnDhs3bmThwoVYa5k0aRJfffUVOTk5JCQkMGvWLMC5p2RERASPPvooX3zxBbGxh96ub+PGjbzxxhvMmDGDKVOm8N577zF16lSmTZvGjBkzGDVqFNOnTz/mR3jyyScxxrBq1SrWrVvHeeedx4YNG3jmmWe4++67ufbaa6murqauro7Zs2cfUmdD039125jR3WP56O7Tie17BqMLfsOi0DPhi/+DmZdDeb7b5YmISCsxZ84c5syZw+DBgxkyZAjr1q1j48aN9O/fn08++YT77ruPefPmERERccxjdenShUGDBgEwdOhQ0tPTKSwspKSkhFGjRgFwzTXXHPM4X3/9NVOnTgWgd+/eJCcns2HDBkaNGsUf/vAH/vSnP5GRkUFwcPBJ1XmiNBLWBkWE+PPENYN54etIrpodyI8jevGTrc9jXjgXrn0Hok/9zvAiIuKio4xYNRVrLffffz+33377Ia8tXbqU2bNn8+CDD3L22Wfz0EMPHfVYgYH7Wix5vd69pyMbyjXXXMOIESOYNWsWF1xwAc8++yxnnXXWCdd5ojQS1kYZY7hlbFdeu3kkL1edyW3mIWrL8mDG2ZA53+3yRESkhQkPD6ekpGTv+vnnn8+LL75IaalzEdj27dvZvXs3O3bsICQkhKlTp3LvvfeydOnSw77/WCIjIwkPD2fBggUAvPnmm8d8z9ixY5k5cyYAGzZsIDMzk169erFlyxa6du3KXXfdxeTJk1m5cuUR62xIGglr40Z3j+WdO0Zx/QteJlX+hvfCHyX4lYvh6jeg25lulyciIi1ETEwMY8aMITU1lQkTJvDnP/+ZtWvX7j1dGBYWxmuvvcamTZu499578Xg8+Pv78/TTTwNw2223MX78eBISEvZOmD+WF154gVtvvRWPx8O4ceOOecrwBz/4AXfeeSf9+/fHz8+Pl19+mcDAQN5++21effVV/P39iY+P54EHHmDRokWHrbMhGWttgx+0MaWlpdnFixe7XUars72wgutfWEBl4S4+ifkrISXpcOVr0PM8t0sTEZHjsHbtWvr06eN2GU2qtLSUsLAwwLkwYOfOnfz97393rZ7DfQfGmCXW2rTD7a/TkQI4Nwp/87ZRBEV24Jz8eymL7AFvXgMbP3G7NBERkcOaNWsWgwYNIjU1lXnz5vHggw+6XdIJUQiTveLCA3n91pEEhMdybu49VET1greug20L3S5NRETkEFdeeSXLly9n9erVzJo1i7i4OLdLOiEKYXKADu2CeOO2kRAUwSVFP6U2LB5mXgG717pdmoiISKuiECaH6BgRzEvThrO9Jpyb6x6g3i8QXrsMSna5XZqIiBxFS5vn3ZqczN+9QpgcVq/4cJ6eOpRv8sJ4KOzX2IoCeGsq1Fa5XZqIiBxGUFAQeXl5CmIusNaSl5dHUFDQCb1PLSrkiE7rEctvJ6fywAerGDHwIS5aPx3++1OY/CQY43Z5IiKyn6SkJLKyssjJyXG7lDYpKCiIpKSkE3qPQpgc1dXDO7Ess4AfL4F+aT+m6/J/QMeBMOLQDsgiIuIef39/unTp4nYZcgJ0OlKOyhjD7y5OpW/HdlyyegwVXc6DOQ/CjmVulyYiItKiKYTJMQX5e3lm6lDq8XBn6c3Y0PbwzjSoLHa7NBERkRZLIUyOS+eYEH43OZUvt9XxftffQGEm/Odu0ARQERGRk6IQJsdt8qAELhzQkfsWhpA99B5Y8z6sfs/tskRERFokhTA5bsYYfn9xf2LDArlu3UjqE9Ng1j1QvNPt0kRERFochTA5IREh/vzp8gFszK3k5fb3OX3DdFpSRETkhCmEyQkb1zOOSwYn8seFtewaMR02fgzLX3e7LBERkRZFIUxOyoMT+xAW6MedG4ZiO42COb+Esjy3yxIREWkxFMLkpMSEBfKrC/uydFsx/+n8c6gqgU9+5XZZIiIiLYZCmJy0SwYnMqprDL/6tp6KYT+E5TMh/Wu3yxIREWkRFMLkpBljeHhSX0qravlzxSSITHbuLambfIuIiByTQpickt7x7Zg6ojMvL9pFxqjfQe4G+PYfbpclIiLS7CmEySn76bk9iQj25xfL22N7XwjzHlXvMBERkWNotBBmjOlkjPnCGPO9MWaNMebuw+xjjDGPG2M2GWNWGmOGNFY90ngiQwK457xeLNiaz5ed74L6Gvjst26XJSIi0qw15khYLXCPtbYvMBL4oTGm70H7TAB6+B63AU83Yj3SiK4e3pne8eH8+pty6obfASteh+1L3S5LRESk2Wq0EGat3WmtXep7XgKsBRIP2m0y8Ip1zAcijTEdG6smaTxej+G+8b3JyCvnnZArITQOPrpfnfRFRESOoEnmhBljUoDBwIKDXkoEtu23nsWhQQ1jzG3GmMXGmMU5OTmNVqecmjN6xTGiSzR/mbuTqtN/Cdvm6wbfIiIiR9DoIcwYEwa8B/zEWlt8Msew1j5nrU2z1qbFxcU1bIHSYIwxTJ/Qm9zSap4rGQUd+sPnv4PaardLExERaXYaNYQZY/xxAthMa+37h9llO9Bpv/Uk3zZpoQZ3jmJ8v3ie+Sqd4jEPQEE6LP2n22WJiIg0O415daQBXgDWWmsfPcJu/wau910lORIostaqt0ELd+/4XlTW1vNYejIkj4G5/w+qSt0uS0REpFlpzJGwMcB1wFnGmOW+xwXGmDuMMXf49pkNbAE2ATOAHzRiPdJEusWFcengRGYuzCR/1P1QthsW6MJXERGR/fk11oGttV8D5hj7WOCHjVWDuOdHZ3Xn/WXb+cfGaB7uNRG+eRzSboaQaLdLExERaRbUMV8aRXJMKJcNSWTmgkxyR/wCqkth3l/dLktERKTZUAiTRvOjM3tQV295YrU/DLwaFs6AIl13ISIiAgph0og6x4Rw2ZBEXl+Yye6hPwVbB1//ze2yREREmgWFMGlUPzqzB/X1lqeWVcPgqU67Co2GiYiIKIRJ43JGw5J4fWEmOYN/BLZeo2EiIiIohEkT+OGZ3amtq+e5FTUaDRMREfFRCJNG1zkmhIsGJjBzQSZFaXf5RsOO1L9XRESkbVAIkyZx5xndKK+u4+U19b7RsFegKMvtskRERFyjECZNond8O87u3Z6Xv91KxcifaG6YiIi0eQph0mR+cGZ3CspreH09+42GaW6YiIi0TQph0mSGJkcxoks0M77aQtWon0B9HXz3hNtliYiIuEIhTJrUD87sTnZxJf/a6oUBU2DJy1CW53ZZIiIiTU4hTJrU6T1iSU1sxzNzt1A3+m6oKYcFz7hdloiISJNTCJMmZYzhB2d0Z2tuGXN2R0LvC2Hhs1BV4nZpIiIiTUohTJrc+f3i6Rwdwox5W2Dsz6CyCBa/6HZZIiIiTUohTJqc12O4+bQuLM0sZEltF+h6Bnz3JNRUul2aiIhIk1EIE1dckZZERLA/M77aCqf9DEp3wYrX3S5LRESkySiEiStCAvyYOrIzH3+fTXr4UEhMg68fg7pat0sTERFpEgph4pobRqXg7/Hw4rfpztywwgxY84HbZYmIiDQJhTBxTft2QVw8OIG3F2+jIOlsiOsD3zwG1rpdmoiISKNTCBNX3TK2K5U19by2YBuM/jHsWg1bvnC7LBERkUanECau6tkhnDN6xfHP7zKo7H0JhMXDt/9wuywREZFGpxAmrrt1bFdyS6v4cHUujLgdNn8O2avdLktERKRRKYSJ60Z3i6Fvx3bMmLeV+iHTwD9UN/YWEZFWTyFMXGeM4dbTu7Bpdylzt9XAkOtg1btQvMPt0kRERBqNQpg0CxP7J9A+PJAXv9kKI+8EWwcLnnW7LBERkUajECbNQoCfh+tGJjNvYy6bamKgzyRY/JJu7C0iIq2WQpg0G9eM6EyAn4eXvkl32lVUFcGy19wuS0REpFEohEmzERMWyOSBCby/dDtF0QOh82j47indykhERFolhTBpVqaN6UJFTR1vLsp0RsOKMmHth26XJSIi0uAUwqRZ6ZvQjpFdo3nluwxqu58HMd2d5q26lZGIiLQyCmHS7Ewb04XthRV8sjbHuVJyxzLYttDtskRERBqUQpg0O+f06UCn6GBngv7AqyEoAhY87XZZIiIiDUohTJodr8dww6gUFqbnszqnFoZcD9//G4qy3C5NRESkwSiESbN0RVonQgK8TvPW4bcBFhY973ZZIiIiDUYhTJqliGB/Lh+axH9X7CTH2wF6T4QlL0N1uduliYiINAiFMGm2bhidQnVdPTMXZMCIO6CiAFa97XZZIiIiDUIhTJqtbnFhnNkrjtfmZ1KVOBI69If5z6hdhYiItAoKYdKsTRvThdzSKmatyoaRd0DOWtg61+2yRERETplCmDRrY3vE0r19GC99k45NvQxCYp3RMBERkRZOIUyaNWMMN4xOYdX2IpbtrIS0abDhI8jf4nZpIiIip0QhTJq9SwcnEh7oxyvfpkPazeDxwoLn3C5LRETklCiESbMXGujHZUOTmLVqJzkmGvpdAsteg8pit0sTERE5aQph0iJcPyqZmjrLmwszYcSdUF0Cy193uywREZGTphAmLULXuDBO7xnHawsyqOk4GJKGwYJnoL7e7dJEREROikKYtBg3jEpmV3EVc9bscpq3FmyFTZ+4XZaIiMhJUQiTFuOMXu3pFB3MP79Lh76TIbwjLHjW7bJEREROikKYtBhej+G6kcks3JrP2t0VkHYTbP4Mcje6XZqIiMgJUwiTFmVKWicC/Ty88l0GDL0RvAGwUO0qRESk5VEIkxYlMiSAiwcl8q9l2ynyREHqZc5VkpVFbpcmIiJyQhTCpMW5fnQyFTV1vLNkGwy/DapL1a5CRERaHIUwaXH6JUQwLCWKV+dnUN9xMCQNd05Jql2FiIi0IAph0iJdPyqFjLxy5m7IgRG3O/eS3PSp22WJiIgcN4UwaZHO7xdP+/DAfe0qwuKd5q0iIiIthEKYtEgBfh6uGdGZL9fnkF5QDcNuVrsKERFpURTCpMW6Znhn/DyGV+erXYWIiLQ8CmHSYrVvF8SE/h15e/E2ygOiod+lvnYVxW6XJiIickwKYdKi3TAqmZLKWv61bAeMULsKERFpORTCpEUbmhxFv4R2vPJdOjZhCCQNg4XPql2FiIg0e40WwowxLxpjdhtjVh/h9TOMMUXGmOW+x0ONVYu0XsYYbhiVwrrsEhZszYcRdzjtKjZ/5nZpIiIiR9WYI2EvA+OPsc88a+0g3+O3jViLtGKTBiUQGeLPK9+lQ59JalchIiItQqOFMGvtV0B+Yx1fZI8gfy9XpnXi4zW72FlWB2k3OY1b1a5CRESaMbfnhI0yxqwwxvzPGNPP5VqkBZs6Mpl6a3l9QSakTQOPPyyc4XZZIiIiR+RmCFsKJFtrBwL/AP51pB2NMbcZYxYbYxbn5OQ0VX3SgnSKDuHs3u15Y2EmVUExkHopLJ+pdhUiItJsuRbCrLXF1tpS3/PZgL8xJvYI+z5nrU2z1qbFxcU1aZ3Sclw/KoXc0mr+tyrbuZ+k2lWIiEgz5loIM8bEG2OM7/lwXy15btUjLd9p3WPpGhvq3E8ycaivXcVzalchIiLNUmO2qHgD+A7oZYzJMsbcbIy5wxhzh2+Xy4HVxpgVwOPAVdZa21j1SOvn8RiuH5XMssxCVmYVwvDbIX+z2lWIiEizZFpa7klLS7OLFy92uwxppkoqaxj5h88Yn9qRv17aBx5LhfgBMPVdt0sTEZE2yBizxFqbdrjX3L46UqRBhQf5c+mQJP6zcgd5lRbSboZNn0DuJrdLExEROYBCmLQ6149Kprq2njcXbYOhN/raVTzndlkiIiIHUAiTVqdHh3DGdI/htfkZ1ITE+dpVvK52FSIi0qwohEmrNG10F3YWVfLxmmxngn51Cax4w+2yRERE9lIIk1bprN7tSY4J4aVv0iFpKCSmwYJn1a5CRESaDYUwaZU8HsMNo1JYklHgtKsYcYevXcXnbpcmIiICKIRJK3ZFWhJhgX7OaFjfyRDWARY843ZZIiIigEKYtGLhQf5cPjSJ/67cwe7yeki7Se0qRESk2VAIk1btxtEp1NZbXluQCUOnOe0qFs1wuywRERGFMGndUmJDOatXe15fkEFVcCz0uwSWzYSqErdLExGRNk4hTFq9aWO6kFtazX9W7HQm6FeXwHK1qxAREXcphEmrN6Z7DD07hPHSN1uxiUMgcSgsVLsKERFxl0KYtHrGGG4c3YU1O4pZlF7gjIblbVK7ChERcZVCmLQJlwxOJDLEn5e+2Qp9L3baVSx81u2yRESkDVMIkzYhOMDLVcM68/GabLJKap0rJTfOgbzNbpcmIiJtlEKYtBnXj0rGGMOr32VAmq9dxcLn3C5LRETaKIUwaTMSIoMZ3y+eNxZmUh6odhUiIuIuhTBpU246LYXiylreWZwFI25XuwoREXGNQpi0KUOToxnSOZLnv95Cbcc97SqeU7sKERFpcgph0ubcdno3tuVX8NGabF+7io3OPSVFRESakEKYtDnn9u1Al9hQnvtqC7bvZGiXCN887nZZIiLSxiiESZvj9RhuGduFlVlFLMgshVE/hIyvIWux26WJiEgbohAmbdJlQ5KICQ3gua+2wJDrISgCvv6b22WJiEgbohAmbVKQv5frR6Xw+brdbCwEht0K62ZB7ka3SxMRkTZCIUzarOtGJRPk72HGvC3OBH1vAHyruWEiItI0FMKkzYoODWBKWif+tWwHu+vDYfC1sOJNKMl2uzQREWkDjiuEGWPuNsa0M44XjDFLjTHnNXZxIo3t5tO6UFtfz0vfpsOoH0F9Lcx/2u2yRESkDTjekbCbrLXFwHlAFHAd8EijVSXSRJJjQhmfGs9r8zMoDUuGPpNg8YtQWex2aSIi0sodbwgzvuUFwKvW2jX7bRNp0W4/vRsllbXMnJ8Bp/0EqophyUtulyUiIq3c8YawJcaYOTgh7GNjTDig+7xIqzCwUyRje8QyY95WKuMGQJdx8N2TUFPhdmkiItKKHW8IuxmYDgyz1pYD/sC0RqtKpIn96Mzu5JZW8daibXD6vVC6C5a+6nZZIiLSih1vCBsFrLfWFhpjpgIPAkWNV5ZI0xrRNYbhKdE8M3cz1UmjofNop3lrbZXbpYmISCt1vCHsaaDcGDMQuAfYDLzSaFWJuOBHZ3VnZ1El7y/bDuPuhZIdsHym22WJiEgrdbwhrNZaa4HJwBPW2ieB8MYrS6Tpje0Ry8CkCJ76cjO1yeMgaRjM+xvU1bhdmoiItELHG8JKjDH347SmmGWM8eDMCxNpNYwx/PDM7mTml/OfVTth3H1QlOk0cBUREWlgxxvCrgSqcPqFZQNJwJ8brSoRl5zTpwO948N54vNN1Hc9GxIGw7y/QF2t26WJiEgrc1whzBe8ZgIRxpgLgUprreaESavj8TijYZtzyvjo+11w+i+gIB1Wv+t2aSIi0soc722LpgALgSuAKcACY8zljVmYiFsu6N+RrrGhPP7ZRup7jIcO/WHu/9NomIiINKjjPR35S5weYTdYa68HhgO/aryyRNzj9RjuOrsH67JLmL0mG868H/I3w4o33C5NRERakeMNYR5r7e791vNO4L0iLc5FAxPo0T6MRz/ZQG338ZA4FL58RH3DRESkwRxvkPrIGPOxMeZGY8yNwCxgduOVJeIur8fws3N7siWnjA9X7ISzH4LiLFise0qKiEjDON6J+fcCzwEDfI/nrLX3NWZhIm4bnxpPv4R2PPbZBmqST4eUsc6VktVlbpcmIiKtwHGfUrTWvmet/Znv8UFjFiXSHBhj+Pl5vdiWX8Hbi7c5o2FlObDgGbdLExGRVuCoIcwYU2KMKT7Mo8QYU9xURYq45YxecQzpHMk/PttEZfxQ6Dkevvk7VBS4XZqIiLRwRw1h1tpwa227wzzCrbXtmqpIEbfsGQ3LLq7k9QWZcNaDUFkE3/7D7dJERKSF0xWOIscwunsso7rG8OQXmyiJ7A2pl8H8p6F4h9uliYhIC6YQJnIcpk/oTV5ZNc/O3QJn/Qrqa+Hz37tdloiItGAKYSLHYWCnSC4amMDzX28h29sRht8Gy2fCzpVulyYiIi2UQpjIcfrF+b2or4e/zlkPp/8cgiNhzoNgrduliYhIC6QQJnKcOkWHcP2oZN5dmsXaQi+Mmw5b58LGT9wuTUREWiCFMJET8KOzuhMe6Mcf/7cO0m6C6G7OaJhu7i0iIidIIUzkBESGBHDX2T34akMO87YWwbm/hdz1sPSfbpcmIiItjEKYyAm6blQynaKD+f2stdT1vACSx8AXf4CKQrdLExGRFkQhTOQEBfp5mT6+D+uyS3h90TYY/0eoyHeCmIiIyHFSCBM5CRf0j2dU1xj+8vF68tv1ceaHLZoB2avcLk1ERFoIhTCRk2CM4TeT+1FaVctf5qx3bmcUHAWz71XLChEROS4KYSInqWeHcK4flcwbCzNZne+Bc34Dmd/ByrfcLk1ERFoAhTCRU/CTc3oSHRLAw/9egx10DSSmwZxfOTf5FhEROQqFMJFTEBHsz33je7Mko4APlu+EiX+Bshz48hG3SxMRkWZOIUzkFF0+NImBnSL5w+x1FEWlQto0WPAs7FzhdmkiItKMNVoIM8a8aIzZbYxZfYTXjTHmcWPMJmPMSmPMkMaqRaQxeTyG/5ucSn5ZFX/6aB2c/RCExsK/f6xO+iIickSNORL2MjD+KK9PAHr4HrcBTzdiLSKNqn9SBDeN6cLrCzJZtMvCBX92RsLmP+V2aSIi0kw1Wgiz1n4F5B9ll8nAK9YxH4g0xnRsrHpEGttPz+1JYmQw97+/iqoeE6H3hU4D1/wtbpcmIiLNkJtzwhKBbfutZ/m2HcIYc5sxZrExZnFOTk6TFCdyokID/fi/i1PZtLuUZ+ZudUbDvP7wn5+od5iIiByiRUzMt9Y+Z61Ns9amxcXFuV2OyBGd2bs9Fw7oyJNfbGJTZTs49zewdS4sf93t0kREpJlxM4RtBzrtt57k2ybSoj10UV+C/D088MEq6gffAJ1Hw8cPQEm226WJiEgz4mYI+zdwve8qyZFAkbV2p4v1iDSI9uFB/HJiHxZuzefVBdtg0j+gtsq5WlKnJUVExKcxW1S8AXwH9DLGZBljbjbG3GGMucO3y2xgC7AJmAH8oLFqEWlqU9I6Ma5nHI/8bx1b6Qjn/Bo2zoGlr7hdmoiINBPGtrD/M09LS7OLFy92uwyRY8ouquS8v82lR4dw3r5tBN7XLobtS+HObyAqxe3yRESkCRhjllhr0w73WouYmC/SEsVHBPHrSf1YklHAC9+kw+SnwHjgXz+A+nq3yxMREZcphIk0oksGJ3Je3w78Zc4GNlZFwvhHIOMbNXEVERGFMJHGZIzh95f0JzTAyz3vrKCm/1XQayJ89lvYtcbt8kRExEUKYSKNLC48kD9c0p+VWUU89tlGuOjvEBwJ70yD6jK3yxMREZcohIk0gQn9O3JlWiee+nIz3+4ycMmzkLsBPprudmkiIuIShTCRJvLwpL50iQ3lp28tJz9+DIz9mdOyYtW7bpcmIiIuUAgTaSIhAX48ftVgCspq+MW7K7HjpkPScOfekvlb3S5PRESamEKYSBNKTYzgvgm9+XTtLl5btAMuex48Hnj3Jqitdrs8ERFpQgphIk3spjEpnNkrjt/NWsuaikiY9ATsWApzful2aSIi0oQUwkSamDGGv1wxkOiQAO58bSlFXSbAqB/BwudgxVtulyciIk1EIUzEBTFhgTx57RB2FFZwz9vLqT/715B8Gvznbti50u3yRESkCSiEibhkaHIUD07sw6drd/P0vAy44iUIjoK3pkJ5vtvliYhII1MIE3HRDaNTmDQwgb/OWc832R6Y8goU74D3b9P9JUVEWjmFMBEXGWP446X96RYXxl1vLGNHeCpM+BNs+gQ++43b5YmISCNSCBNxWWigH09PHUp1bT23vrKY8gHXQ9pN8M1jsGym2+WJiEgjUQgTaQa6tw/j8asH8/3OYn7+7krqz/8TdBnnTNTP+Nbt8kREpBEohIk0E2f2bs8DE/owe1U2j89Nhyn/hKgUePNayN/idnkiItLAFMJEmpFbxnbh8qFJPPbpRmZtrIRr3gIsvH4VVBS6XZ6IiDQghTCRZsQYw+8vSWVI50jueWc5Kyti4MrXnJGwN6+Fmkq3SxQRkQaiECbSzAT6eXn2ujRiwwK56eVFZIYPgUuegYyv4f1bob7O7RJFRKQBKISJNENx4YG8PG04NXWWG19aSEHXSXD+H2Dtv+F/vwBr3S5RREROkUKYSDPVvX0Yz9+QRlZhBbe8spjKtDtg9F2w6Hn46s9ulyciIqdIIUykGRuWEs3frxzE0swC7n5zGXVn/xoGXAVf/N4JYyIi0mIphIk0cxP6d+RXE/vy8ZpdPPjh99hJ/4Ce42HWPbD0VbfLExGRk+TndgEicmw3ndaF3NIqnvpyM2GBXh644mXMm9fAv38MfoEwYIrbJYqIyAlSCBNpIe49vxdlVbXMmLeVsEB/7r5yJrw+BT64Hbz+0O8St0sUEZEToBAm0kIYY3j4on6UVdfxt083EBro5Zar34TXLoP3bgGPH/S5yO0yRUTkOGlOmEgL4vEYHrm0Pxf0j+f/Zq3ljRX5cO07kDAY3r4BVr7jdokiInKcFMJEWhg/r4fHrhzMmb3ieOCDVbyxshCu+wA6j3KauS59xe0SRUTkOCiEibRAAX4enp46lDN6xnH/+6t4dWmeMyLW/Wxnsv78Z9wuUUREjkEhTKSFCvL38sx1QzmnTwd+9eEaXly4C656HXpfCB/dB/P+qs76IiLNmEKYSAsW6OflqWuHML5fPL/97/c8920WXPFP6D8FPvstfDRd95oUEWmmdHWkSAsX4OfhH9cM5qdvLecPs9dRXl3H3Zc8gwmNg/lPQvF2uHQG+Ae7XaqIiOxHIUykFfD3enjsykEE+Xt57NON5JVW8+tJv8cb2Qk+uh9emQxXvQGhMW6XKiIiPgphIq2En9fDny8fQExYAM/O3UJeWRV/u/I2AtslwHu3wgvnwtR3Ibqr26WKiAiaEybSqhhjuH9CHx6c2IfZq7K58cVFlHS9AG74N1Tkw4yzYMuXbpcpIiIohIm0SreM7crfrhzIovR8rnx2PtkRg+CWzyCsA7x6Kcx/WldOioi4TCFMpJW6ZHASL9w4jMz8ciY98TUrK2Lglk+h1wTnqskPfwg1lW6XKSLSZimEibRi43rG8d6do/H3epjy7HfM3lAKU16FcdNh+Ux4eSIUZbldpohIm6QQJtLK9YoP58MfjaFfQgQ/mLmUJ77cjD1jOlz5GuSsh2dOg/UfuV2miEiboxAm0gbEhgUy85YRXDI4kb/M2cBdby6nrOsEuH0uRCTBG1fCnAehrsbtUkVE2gyFMJE2Isjfy6NTBnLf+N7MWrmDS576hi31HeDmT2HYLfDtP+ClCVCY6XapIiJtgkKYSBtijOHOM7rx6s0jyC2tZtIT3/DR+gKY+Fe44mXYvQ6eHgMr3tTVkyIijUwhTKQNGtM9lv/++DS6tw/jjteW8ofZa6npPRnumAft+8IHt8Pb10NZntulioi0WgphIm1UQmQwb90+kutGJvPcV1u44pnvyLQdYNpsOOfXsP5/8PQo2DDH7VJFRFolhTCRNizQz8vvLk7liWsGszmnlAsen8e/VmTDaT+F276AkFh4/Qr44E4oz3e7XBGRVkUhTES4cEAC/7t7LL3jw/nJW8v56VvLKYns7QSxsffAqrfhiWGw8h3NFRMRaSAKYSICQFJUCG/eNpKfnNODD5dvZ8Lf5/Ftegmc/RDcNheikuH9W2DmFbqCUkSkASiEichefl4PPzmnJ+/cMQp/r4drnl/Ar/61mrKo3nDzJzD+Ecj4Fp4cCV8/BrXVbpcsItJiKYSJyCGGJkcz+66x3HxaF15bkMH5j33Ft1sLYOSd8MP50OV0+PRhZ+L+xk/dLldEpEVSCBORwwoO8PKrC/vy9u2+UbEZC/jlB6soCuwI17wJ177rzA+beRm8cTXkb3G7ZBGRFsXYFjbJNi0tzS5evNjtMkTalIrqOv46Zz0vfrOV6NBAHpzYh8mDEjB1NTD/Kfjqz84tj0beAaf9DIIj3S5ZRKRZMMYssdamHfY1hTAROV6rtxfxyw9WsSKriNHdYvjdxal0iwuD4p3w2W+cTvtBEXD6z2HYreAf5HbJIiKuUggTkQZTV295fWEm/++jdVTV1HP7uK7ceUY3QgL8IHsVfPIwbP4MIjrDWQ9C/yvAo5kPItI2KYSJSIPbXVLJ72et5cPlO+jQLpB7z+/NpYMT8XgMbP4CPnkIsldCfH844wHoNQGMcbtsEZEmpRAmIo1mcXo+v/vv96zIKmJAUgS/urAvw1Kiob4eVr8LX/weCtKh40A4437oOV5hTETaDIUwEWlU9fWWD1ds50//W092cSUT+3fkvvG96RwT4kzYX/kWzP1/UJgBCYOdMNbjPIUxEWn1FMJEpEmUV9fy3FdbeHbuFmrq6rlqeCd+fFYPOrQLcsLYijedKyn3hLHTfga9J4LH63bpIiKNQiFMRJrUruJKnvh8E28szMTrMdw4OoU7xnUjKjTAF8begHl/dU5TxnSH0T+GgVeDX6DbpYuINCiFMBFxRWZeOY99toEPlm0nNMCPW8d25abTUggP8oe6Wlj7b/jmMdi5AsI6OB35025y2lyIiLQCroUwY8x44O+AF3jeWvvIQa/fCPwZ2O7b9IS19vmjHVMhTKTl2bCrhEfnbOCjNdlEBPtz4+gUpo1JITIkwOm6v+VL+ObvsOULCAiHoTfA8Nucm4aLiLRgroQwY4wX2ACcC2QBi4CrrbXf77fPjUCatfZHx3tchTCRlmtlViFPfL6JOd/vIjTAy9RRydxyWlfiwn2nIXcsd8LY9x8CFnpOgBG3O/eq1CR+EWmB3Apho4BfW2vP963fD2Ct/eN++9yIQphIm7Muu5invtjMf1fuwN/r4erhnbnt9K4kRAY7OxRth8UvwOKXoCIf4vo4YWzAlRAQ4m7xIiInwK0Qdjkw3lp7i2/9OmDE/oHLF8L+COTgjJr91Fq77TDHug24DaBz585DMzIyGqVmEWlaW3PLePrLTby/dDvGwEUDErh5bBf6JfjmhNVUwOr3YMEzTjf+oEgYPBWGToPY7q7WLiJyPJpzCIsBSq21VcaY24ErrbVnHe24GgkTaX2yCsp5ft5W3l68jfLqOkZ3i+GWsV04o2d7pwO/tZA53wlj6/4L9bWQfJozd6zPJN2jUkSarWZ7OvKg/b1AvrX2qJdFKYSJtF5FFTW8uTCTl79NZ2dRJV3jQrn5tC5cOjiJ4ABfL7GSXbB8Jix9BQq2QnAUDLjKCWTt+7j7AUREDuJWCPPDOcV4Ns7Vj4uAa6y1a/bbp6O1dqfv+SXAfdbakUc7rkKYSOtXU1fP7FU7eX7eVlZtLyIqxJ8pwzpx7fBkpws/OLdFSv8KlvwT1v4H6mug0wjndGXfyWpzISLNgpstKi4AHsNpUfGitfb3xpjfAouttf82xvwRmATUAvnAndbadUc7pkKYSNthrWVRegEvfL2FT9fupt5axvWMY+qIZM7s3R6vx3fFZFmu0wB26SuQuwH8gpxO/AOvhq5ngtfP3Q8iIm2WmrWKSIuXXVTJGwszeWNhJrtLqkiMDOaaEZ2ZktZpX4sLa2H7UieQrX4XKgogtD0MmAIDr4L4/u5+CBFpcxTCRKTVqKmr59Pvd/Hq/Ay+3ZyHv9dwfr94rhzWiTHdYp2J/AC11bBxjhPINnzsnK7skAr9r4B+l6gRrIg0CYUwEWmVNueU8tr8DN5fup2iihoSI4O5bGgSVwxNolP0fv3EyvOdVhcr3oTtvn8/EtMg9VInkLVLcOcDiEirpxAmIq1aZU0dn67dxduLs5i3MQdrYXS3GK5IS2J8v477rqwEyN8Kaz6ANe87vccw0HmUE8j6Toaw9q59DhFpfRTCRKTN2FFYwXtLsnhnSRaZ+eWEB/px4cAELhmcSFpy1L7TlQC5m5wwtvp9yFkLxgMppzm9x3pP1AiZiJwyhTARaXPq6y0L0/N5e/E2/rcqm4qaOhIjg5k0KIGLByXSKz78wDfs+t4JZGs+gLxNzrbENOhzIfS+SB36ReSkKISJSJtWVlXLnO+z+XD5DuZtzKWu3tI7PpyLBycyaWDCvntW7pGz3uk9tu6/sGOZsy2uN/S+0AllHQfphuIiclwUwkREfHJLq5i1cif/Wr6dZZmFGAPDU6K5aGAC41PjiQ0LPPANhdtg3SwnkGV8A7YeIjpBz/Ohx/nQZSz4Bx/+DxORNk8hTETkMDLyyvhw+Q7+tXw7W3LK8BgY0SWGC/rHc36/eNq3O+ielGV5sOF/Tijb8iXUlINfMHQ5HXqe54SyyE6ufBYRaZ4UwkREjsJay/pdJcxelc3sVTvZtLsUY2BYcjQX9I9nfGpH4iMOCmQ1lZDxNWyYAxs/hoJ0Z3tcn32BrNMIdesXaeMUwkRETsDG/QLZ+l0lAAxNjmJCqjNCdkAPMnA69edudMLYho8h8zuor4XACOd0ZdczoNtZEN1Vc8lE2hiFMBGRk7Rpdykfrd7JrFXZrN1ZDEDv+HDO6dOBc/p2YEBixIFtLwAqi2HLF7DxE+e0ZdE2Z3tEZ+h2hnM/y65nQEh0U34UEXGBQpiISANIzy3j07W7+HTtLhalF1BXb2kfHsjZfTpwbt/2jO4WS5C/98A3WQv5W2Dz504g2/oVVBUDBjoO9I2SnQlJwyEg5DB/qoi0ZAphIiINrKCsmi837ObT73czd0MOpVW1BPt7GdsjlnP6dGBcrzg6HDyxH6CuFnYsdQLZ5i8ga6Fz6tLjD4lDIWUMJI9x5pMFhjX55xKRhqUQJiLSiKpq61iwJd8ZJft+FzuKKgHntOUZvdozrmccQ5OjCPDzHObNJZDxnTPJP/0bpy+ZrQPjhYRBTiBLOQ06j4SgiKb9YCJyyhTCRESaiLWWddklzN2Qw9z1OSzOyKemzhIa4GV091jG9YxjXM+4Qyf371FVCtsWOD3J0r+B7Uugvsa5pVJ8f+g0EjoNh6RhENlZE/1FmjmFMBERl5RW1fLtplzmbsjhy/U5bC+sAKBbXCin94zjtO6xDO8STXiQ/+EPUF0OWYsg41snmG1f4vQnAwiLh07DnPlknUY4c8z8D3MKVERcoxAmItIMWGvZklvGl+tzmLshhwVb8qiqrcfrMQxIimB0txjGdItlSHLUoRP896irhd1rYNtC55G1cF+PMo+/E8T2jJQlDIaoFI2WibhIIUxEpBmqrKljaWYB323O45tNuazIKqKu3hLg5yEtOYrR3WIY1S2WgUkR+HkPM59sj9LdzmjZtgWwbZEz8b/WmZdGUKQztyxhsPPoOEinMUWakEKYiEgLUFpVy6Kt+XyzKZdvN+fxva8vWVigH0OSoxieEkVaSjSDOkUeeaQMoK4Gdq2GHcth53Jnsv+u7525ZQDB0b5QNshZxg9QMBNpJAphIiItUH5ZNfO35PHt5lwWbS3Y270/wOuhf1IEw1KiGd4liqHJ0UQEH2FO2R61VbBrjRPIdixzwtmu750rMQEC20GHftAhdb9lXwgIbdwPKdLKKYSJiLQCheXVLE4vYFF6PgvT81mVVURtvcUY6NUhnGEp0aSlRDGkcxRJUcGYY41s1VQ4wSx7pbPc86gq9u1gILqLL5DtCWd9ITIZPEcZiRORvRTCRERaoYrqOpZtK2DR1gIWZ+SzJKOA8mpnZCs2LIBBnSIZ1CmSwZ2jGJAUceQrMPdnLRRmOqczd61xltmrna7/+P574RcEMT0grte+R2wv596YfgGN94FFWiCFMBGRNqC2rp512SUs21bI8sxClm0rYEtOGeBM9+rRPswXzKIY3DmSHu3Djj7hf3/VZbB7rfPIWQe5G5xlYea+fTx+ThDbE8riekNMN+ehRrPSRimEiYi0UUXlNSzPckLZ8m0FLNtWSGG5M0E/yN9Dn47tSE2IIDWxHf0SIujZIfzwnf2PpLoMcjdCznrIXe8sc9Y7I2d75psBhMQ6AS2mG0R3g5iuznp0Nwhq18CfWqT5UAgTERHA6VWWkVfO8m2FrNpexOrtRXy/o5iSqloA/L2GXvHhpCZE0C8xgtSEdvTp2O7oV2MeTm0V5G2G/M37Lbc44axkx4H7hsY5YSy6qxPOIlOcqzWjkiGsg67alBZNIUxERI6ovt6SmV/O6h1FrN5ezJodTjgr8I2YeT2GrrGh9IoPp3d8OL3i29GrQzhJUcF4PCcRkKrLIH+rE8zyt/hC2p6AtvPAff2CIKLTvlAW2dm5MCAy2VkPiVFIk2ZNIUxERE6ItZYdRZWs9o2Wrd1ZwvpdxWzLr9i7T2iAlx4d9gQz36NDODFhgSf/B1eXQeE2Z65ZYYbzKMjYt15RcOD+/qG+YNYJ2iU6j4jEfc/bJUDAEe7TKdIEFMJERKRBlFbVsmFXCeuznce67GLWZ5fsHTUD58rMbnFhdGsf5izjQukWF0Zi5EmOnO2vstgXyPaEtEwnpBVlQvEOKM879D3BUdAuyQlk+we0iP2Cmn/wqdUlcgQKYSIi0misteSUVu0NZht2lbA5p4xNu0spqtgXzoL8PXSJ3RfKuvtCWpfYUIIDGqjvWE2FE8aKt0PRdmdZvN3Ztme9Iv/Q9wVFODdED+9w0DLemZe2ZxkYrtOfckIUwkREpMlZa8kvq2ZzThmbc0rZvLvUWeaUsa2gnP3/8xPfLojkmBDfI5TkmBBSYkLpHBNCu+Ppb3YiqsuduWdFWb7AlgUlu6A0+8BlXdWh7/UPOTCU7VmGdXAuMAiN3bfU6JqgECYiIs1MZU0d6XllbN7tBLSMvHIy8srIyC8np+TA8BMdGkDn6P0CWnQIKbEhJEWFEBcWeOqnOA/HWqgsPHw4O3hZXXL4YwSE+0JZ7EEBzfcIiTnwudev4T+HuE4hTEREWoyyqloy832hLK+c9LxyMvPLSM8tZ2dRBfX7/WcrwOshITKIxKhgkiJDnGVUMImRwSRGBRPfLuj4G9KerOoyKN0FZXlQlrPfI9dZlufue16We2D/tP0FRztBLTgaQqJ9y6iD1g9a6g4FzZ5CmIiItApVtXVkFVSQmVdOVkE5WYUVbC+oIKuggu2FFYeMonk9hvh2QXvDWVJkMB0jg4mPCCK+XRAdI4KICPY/9n02G0p9vTPCtjeUHSawlec7V4GW5zvz12orj3y8gLDDh7XgqAOfB0VAcKSzDIoE/6Cm+bxy1BCmsU8REWkxAv28visuww77emVNHTsKnUCWVeAENOd5OfM355FdXHnASJpzTA8dI4Lo4AtlHSKC6NguiPi924KJCw/E2xCnPT0eJxyFRENcz+N7T3W5E8b2hLK9y4JDt+9p41FRyN57fR6ON/DQYHbA+uG27feabuDeIBTCRESk1Qjy99I1LoyuRwhpNXX17C6pIruo0nkUV5JdVEF2cRXZRRUsySxgV1EV1XX1B7zPY6B9uBPQ2ocHEhceSFyYs9y7Hh5IbFjgid9d4FgCQpxHRNLxv6e+DiqLnGBWWeg8KgqdbZV7lkX7tpXnOs1z96wf6ZTpHoHt9gWzwHDn1lOB4Qc9Ig5cD2rnvG/Pul9Qm7/SVCFMRETaDH+vx5kvFnnkKxfr6y355dUHBbV9y8y8cpZkFJBfVn3Y97cL8tsbyuLCgw4b1mLCAogOCWi8+Woe774RtxNlrTPPbU9YO1p4qyyEqhLnatPcDU4ft6qSw19ZekiNfvuFsnYHhbWDw1yYc+p1zzIgDAJCndcDQsHvFBoEu0ghTEREZD8ejyE2zBnVSk2MOOJ+NXX15JVWk1NSRU5ppbPc8yh1lquyCskpqaKs+vAjSxHB/sSEBhDte8SE7XkeeNjtgX5NcBrQGCfsBIad2Ojb/mqroKoUqoqcULbnUVkMVcX7bSs+8PU9YW7P+vGEOQCP/4Gh7OCQdqT12J7QccDJfcYGoBAmIiJyEvy9HmeCf0QQcOSwBs4Vn7m+YLa7pIq8smryS6vJL6si1/c8I6+cpZmFFJRXU3fwxDWfsEC/fcHMt4wKDSAyxJ/IYN9yv+dRIQEE+Xua7sKDPfwCnUdozKkdp7Zqv7BW6ozQVZc5bUGqy3zb9jwOs16Wu+95demhFzkMuwUm/vXUajwFCmEiIiKNLDTQj9BAP5JjQo+5b329pbiyxglqZdXklTrL/LKqvdvyy6qde3vuKKKwvIaq2vojHi/Az0NksBPIIkL89z6PDPEnwhfUIoP3e+4Lca6Et4PtDXOxDXO8utoDQ1nAsb+PxqQQJiIi0ox4PIbIkAAiQwLoFnd876msqaOwvIaC8moKy2soqqimoLyGwvIaCiuqKSzzLctryMgrZ0VWIQXlNVQfJbz5eQztgv1pF+TnW/rTLtjPtzzadmc92N/rfog7mNfPudozONLtSgCFMBERkRYvyN9LfITXd2r0+FVU1+0NZwXl1RSV11BQXkNxZQ3FFXuWtXvXs4sr926vrDlygIOjh7jwID/CAv0JC/Ij3DdKGBbkR1ign+8137ZAv4ZpDdJMKYSJiIi0UcEBXoIDgukYceL3uayqraOkstYXymoPG9pOJcTtERLgJWy/kLb3EXTg83DfMjRgz7oT8kIDnfc3x5E5hTARERE5YYF+XgLDvMSGnVx7iNq6esqq6iipqqG0qpbSylpneYTnJb7nZVW1ZJaVU7LfPke6kGF/xkBogBPKnKUfFw7oyO3jup1U/Q1BIUxERESanJ/XQ0SIh4gQ/1M6jrWWqtr6faFs/wBXVeMEt+o6ynzbyqvqKK12wlygXyPfV/QYFMJERESkxTLGEOTvJcjfS1x4y2ra6m4EFBEREWmjFMJEREREXKAQJiIiIuIChTARERERFyiEiYiIiLhAIUxERETEBQphIiIiIi5QCBMRERFxgUKYiIiIiAsUwkRERERcoBAmIiIi4gKFMBEREREXKISJiIiIuMBYa92u4YQYY3KAjCb4o2KB3Cb4c+T46TtpnvS9ND/6TponfS/NT1N8J8nW2rjDvdDiQlhTMcYsttamuV2H7KPvpHnS99L86DtpnvS9ND9ufyc6HSkiIiLiAoUwERERERcohB3Zc24XIIfQd9I86XtpfvSdNE/6XpofV78TzQkTERERcYFGwkRERERcoBB2EGPMeGPMemPMJmPMdLfraSuMMZ2MMV8YY743xqwxxtzt2x5tjPnEGLPRt4zybTfGmMd939NKY8wQdz9B62aM8Rpjlhlj/utb72KMWeD7+3/LGBPg2x7oW9/kez3F1cJbMWNMpDHmXWPMOmPMWmPMKP1e3GWM+anv36/Vxpg3jDFB+q00PWPMi8aY3caY1fttO+HfhjHmBt/+G40xNzRGrQph+zHGeIEngQlAX+BqY0xfd6tqM2qBe6y1fYGRwA99f/fTgc+stT2Az3zr4HxHPXyP24Cnm77kNuVuYO1+638C/mat7Q4UADf7tt8MFPi2/823nzSOvwMfWWt7AwNxvh/9XlxijEkE7gLSrLWpgBe4Cv1W3PAyMP6gbSf02zDGRAMPAyOA4cDDe4JbQ1IIO9BwYJO1dou1thp4E5jsck1tgrV2p7V2qe95Cc5/UBJx/v7/6dvtn8DFvueTgVesYz4QaYzp2LRVtw3GmCRgIvC8b90AZwHv+nY5+HvZ8329C5zt218akDEmAjgdeAHAWlttrS1Evxe3+QHBxhg/IATYiX4rTc5a+xWQf9DmE/1tnA98Yq3Nt9YWAJ9waLA7ZQphB0oEtu23nuXbJk3INyw/GFgAdLDW7vS9lA108D3Xd9V0HgN+AdT71mOAQmttrW99/7/7vd+L7/Ui3/7SsLoAOcBLvtPEzxtjQtHvxTXW2u3AX4BMnPBVBCxBv5Xm4kR/G03ym1EIk2bFGBMGvAf8xFpbvP9r1rmUV5fzNiFjzIXAbmvtErdrkQP4AUOAp621g4Ey9p1eAfR7aWq+U1WTcQJyAhBKI4ycyKlrTr8NhbADbQc67bee5NsmTcAY448TwGZaa9/3bd6157SJb7nbt13fVdMYA0wyxqTjnJ4/C2cuUqTvlAsc+He/93vxvR4B5DVlwW1EFpBlrV3gW38XJ5Tp9+Kec4Ct1toca20N8D7O70e/lebhRH8bTfKbUQg70CKgh+9qlgCcSZX/drmmNsE3F+IFYK219tH9Xvo3sOeqlBuAD/fbfr3vypaRQNF+Q83SQKy191trk6y1KTi/h8+ttdcCXwCX+3Y7+HvZ831d7tu/WfwfZ2tirc0Gthljevk2nQ18j34vbsoERhpjQnz/nu35TvRbaR5O9LfxMXCeMSbKN8p5nm9bg1Kz1oMYYy7AmQPjBV601v7e3YraBmPMacA8YBX75h49gDMv7G2gM5ABTLHW5vv+kXsCZ7i/HJhmrV3c5IW3IcaYM4CfW2svNMZ0xRkZiwaWAVOttVXGmCDgVZw5ffnAVdbaLS6V3KoZYwbhXCwRAGwBpuH8j7V+Ly4xxvwGuBLnau9lwC0484j0W2lCxpg3gDOAWGAXzlWO/+IEfxvGmJtw/jsE8Htr7UsNXqtCmIiIiEjT0+lIERERERcohImIiIi4QCFMRERExAUKYSIiIiIuUAgTERERcYFCmIjIcTLGnGGM+a/bdYhI66AQJiIiIuIChTARaXWMMVONMQuNMcuNMc8aY7zGmFJjzN+MMWuMMZ8ZY+J8+w4yxsw3xqw0xnzg646NMaa7MeZTY8wKY8xSY0w33+HDjDHvGmPWGWNm+po9ioicMIUwEWlVjDF9cLqWj7HWDgLqgGtxbqi82FrbD5iL00Ub4BXgPmvtAJw7NuzZPhN40lo7EBgN7LnNz2DgJ0BfoCvO/QFFRE6Y37F3ERFpUc4GhgKLfINUwTg3660H3vLt8xrwvjEmAoi01s71bf8n8I4xJhxItNZ+AGCtrQTwHW+htTbLt74cSAG+bvRPJSKtjkKYiLQ2Bvintfb+AzYa86uD9jvZe7ZV7fe8Dv07KiInSacjRaS1+Qy43BjTHsAYE22MScb59+5y3z7XAF9ba4uAAmPMWN/264C51toSIMsYc7HvGIHGmJCm/BAi0vrp/+BEpFWx1n5vjHkQmGOM8QA1wA+BMmC477XdOPPGAG4AnvGFrC3ANN/264BnjTG/9R3jiib8GCLSBhhrT3ZEXkSk5TDGlFprw9yuQ0RkD52OFBEREXGBRsJEREREXKCRMBEREREXKISJiIiIuEAhTERERMQFCmEiIiIiLlAIExEREXGBQpiIiIiIC/4/yi2z4Ab33+oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更换数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们换一个数据集，使用MNIST手写数字数据集。\n",
    "\n",
    "MNIST是最有名的手写数字数据集之一，主页：http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "MNIST手写数字数据集有60000个样本组成的训练集，10000个样本组成的测试集，是NIST的子集。数字的尺寸都是归一化后的，且都在图像的中央。可以从上方的主页下载。\n",
    "\n",
    "我们使用的数据集是kaggle手写数字识别比赛中的训练集。数据集一共42000行，785列，其中第1列是标记，第2列到第785列是图像从左上角到右下角的像素值。图像大小为28×28像素，单通道的灰度图像。\n",
    "\n",
    "我们使用的是kaggle提供的MNIST手写数字识别比赛的训练集。这个数据集还是手写数字的图片，只不过像素变成了 $28 \\times 28$，图片的尺寸变大了，而且数据集的样本量也大了。我们取30%为测试集，70%为训练集。训练集样本数有29400个，测试集12600个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19128/2016476978.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (Temp/ipykernel_19128/172503953.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\CKXG\\AppData\\Local\\Temp/ipykernel_19128/172503953.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    data = pd.read_csv(\"C:\\Users\\CKXG\\Documents\\CODE\\Homework\\machineLearning\\Chapter5\\data\\mnist_test.csv\")\u001b[0m\n\u001b[1;37m                                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "\n",
    "data = pd.read_csv(\"C:\\Users\\CKXG\\Documents\\CODE\\Homework\\machineLearning\\Chapter5\\data\\mnist_test.csv\")\n",
    "X = data.values[:, 1:].astype('float32')\n",
    "Y = data.values[:, 0]\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.3, random_state = 32)\n",
    "\n",
    "trainY_mat = np.zeros((len(trainY), 10))\n",
    "trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n",
    "\n",
    "testY_mat = np.zeros((len(testY), 10))\n",
    "testY_mat[np.arange(0, len(testY), 1), testY] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainX.shape, trainY.shape, trainY_mat.shape, testX.shape, testY.shape, testY_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制训练集前10个图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_, figs = plt.subplots(1, 10, figsize=(8, 4))\n",
    "for f, img, lbl in zip(figs, trainX[:10], trainY[:10]):\n",
    "    f.imshow(img.reshape((28, 28)), cmap = 'gray')\n",
    "    f.set_title(lbl)\n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test：请你使用kaggle MNIST数据集，根据下表设定各个超参数，计算测试集上的精度，绘制损失值变化曲线，填写下表\n",
    "\n",
    "任务流程：\n",
    "1. 对数据集进行标准化处理\n",
    "2. 设定学习率和迭代轮数进行训练\n",
    "3. 计算测试集精度\n",
    "4. 绘制曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 双击此处填写\n",
    "\n",
    "精度保留4位小数；训练时间单位为秒，保留两位小数。\n",
    "\n",
    "隐藏层单元数 | 学习率 | 迭代轮数 | 测试集精度 | 训练时间(秒)\n",
    "-|-|-|-|\n",
    "100 | 0.1 | 50 |  | \n",
    "100 | 0.1 | 100 |  | \n",
    "100 | 0.1 | 150 |  | \n",
    "100 | 0.1 | 500 |  | \n",
    "100 | 0.01 | 500 |  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
